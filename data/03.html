<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>表現</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 0px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    <style>
        .thin-line {
            margin: 0; 
            margin-left:2em;
            border: 0;
            height: 1px;
            background-color: gray;
        }

        .thick-line {
            margin: 0; 
            margin-left:2em;
            border: 0;
            height: 2px;
            background-color: black;
        }
    </style>
    <style>
        .blue {
            color: blue; /* 好きな色に変更してください */
        }
    </style>
    </head>
    <body>
        <h1><center>Ⅲ REPRESENTATIONS<br>表現</center></h1>
<h2>14. Basic vectors</h2>
<p>
In the preceding chapters we set up an algebraic scheme involving
certain abstract quantities of three kinds, namely bra vectors, ket
vectors, and linear operators, and we expressed some of the funda-
mental laws of quantum mechanics in terms of them. It would be
possible to continue to develop the theory in terms of these abstract
quantities and to use them for applications to particular problems.
However, for some purposes it is more convenient to replace the
abstract quantities by sets of numbers with analogous mathematical
properties and to work in terms of these sets of numbers. The proce-
dure is similar to using coordinates in geometry, and has the advan-
tage of giving one greater mathematical power for the solving of
particular problems.

</p><p>
The way in which the abstract quantities are to be replaced by
numbers is not unique, there being many possible ways corresponding
to the many systems of coordinates one can have in geometry. Hach
of these ways is called a representation and the set of numbers that
replace an abstract quantity is called the representative of that
abstract quantity in the representation. Thus the representative of
an abstract quantity corresponds to the coordinates of a geometrical
object. When one has a particular problem to work out in quantum
mechanics, one can minimize the labour by using a representation
in which the representatives of the more important abstract quanti-
ties occurring in that problem are as simple as possible.

</p><p>
To set up a representation in a general way, we take a complete
set of bra vectors, i.e. a set such that any bra can be expressed
linearly in terms of them (as a sum or an integral or possibly an
integral plus a sum). These bras we call the basic bras of the repre-
sentation. They are sufficient, as we shall see, to fix the representation
completely.

</p><p>
Take any ket \(|a\rangle\) and form its scalar product with each of the basic
bras. The numbers so obtained constitute the representative of \(|a\rangle\).
They are sufficient to determine the ket \(|a\rangle\) completely, since if there
is a second ket, \(|a_1\rangle\) say, for which these numbers are the same, the
difference \(|a\rangle-|a_1\rangle\) will have its scalar product with any basic bra
vanishing, and hence its scalar product with any bra whatever will
vanish and \(|a\rangle-|a_1\rangle\) itself will vanish.

</p><p>
We may suppose the basic bras to be labelled by one or more
parameters, \(\lambda_1,\lambda_2,...,\lambda_u,\) each of which may take on certain numerical
values. The basic bras will then be written \(\langle\lambda_1,\lambda_2...\lambda_u|\) and the repre-
sentative of \(|a\rangle\) will be written \(\langle\lambda_1\lambda_2...\lambda_u|a\rangle\). This representative will
now consist of a set of numbers, one for each set. of values that
\(\lambda_1,\lambda_2,...,\lambda_u\) may have in their respective domains. Such a set of
numbers just forms a function of the variables \(\lambda_1,\lambda_2,...,\lambda_u\). Thus the
representative of a ket may be looked upon either as a set of numbers
or as a function of the variables used to label the basic bras.

</p><p>
If the number of independent states of our dynamical system is
finite, equal to \(n\) say, it is sufficient to take n basic bras, which may
be labelled by a single parameter \(\lambda\) taking on the values \(1,2,3,...,n\).
The representative of any ket \(|a\rangle\) now consists of the set of \(n\) numbers
\(\langle 1|a\rangle,\langle 2|a\rangle,\langle 3|a\rangle,...,\langle n|a\rangle\), which are precisely the coordinates of
the vector \(|a\rangle\) referred to a system of coordinates in the usual way.
The idea of the representative of a ket vector is just a generalization
of the idea of the coordinates of an ordinary vector and reduces to
the latter when the number of dimensions of the space of the ket
vectors is finite.

</p><p>
In a general representation there is no need for the basic bras to
be all independent. In most representations used in practice, how-
ever, they are all independent, and also satisfy the more stringent

condition that any two of them are orthogonal. The representation
is then called an orthogonal representation.

</p><p>
Take an orthogonal representation with basic bras \(\langle\lambda_1\lambda_2...\lambda_u|\), labelled by parameters \(\lambda_1,\lambda_2,...,\lambda_u\) whose domains are all real. Take
a ket \(|a\rangle\) and form its representative \(\langle\lambda_1\lambda_2...\lambda_u|a\rangle\). Now form the
numbers \(\lambda_1\langle\lambda_1\lambda_2...\lambda_u|a\rangle\) and consider them as the representative of
a new ket \(|b\rangle\). This is permissible since the numbers forming the
representative of a ket are independent, on account of the basic bras
being independent. The ket \(|b\rangle\) is defined by the equation

\[
\langle\lambda_1\lambda_2...\lambda_u|b\rangle = \lambda_1\langle\lambda_1\lambda_2...\lambda_u|a\rangle
\]

The ket \(|b\rangle\) is evidently a linear function of the ket \(|a\rangle\), so it may
be considered as the result of a linear operator applied to \(|a\rangle\). Calling
this linear operator \(L_1\), we have

\[
|b\rangle = L_1|a\rangle
\]

and hence 

\[
\langle\lambda_1\lambda_2...\lambda_u|L_1|a\rangle = \lambda_1\langle\lambda_1\lambda_2...\lambda_u|a\rangle
\]

This equation holds for any ket \(|a\rangle\), so we get

\[
\langle\lambda_1\lambda_2...\lambda_u|L_1 = \lambda_1\langle\lambda_1\lambda_2...\lambda_u|  \tag{1}
\]

Equation (1) may be looked upon as the definition of the linear
operator \(L_1\). It shows that each basic bra is an etgenbra of \(L_1\), the
value of the parameter \(\lambda_1\) being the eigenvalue belonging to it.

</p><p>
From the condition that the basic bras are orthogonal we can
deduce that \(L_1\), is real and is an observable. Let \(\lambda_1^\prime,\lambda_2^\prime,...,\lambda_u^\prime\) and
\(\lambda_1^{\prime\prime},\lambda_2^{\prime\prime},...,\lambda_u^{\prime\prime}\) be two sets of values for the parameters \(\lambda_1,\lambda_2,...,\lambda_u\). We have, putting \(\lambda^\prime\)'s for the \(\lambda^\prime\)'s in (1) and multiplying on the right
by \(|\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}\rangle\), the conjugate imaginary of the basic bra \(\langle\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}|\),

\[
\langle\lambda_1^\prime\lambda_2^\prime...\lambda_u^\prime|L_1|\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}\rangle = \lambda_1^\prime\langle\lambda_1^\prime\lambda_2^\prime...\lambda_u^\prime|\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}\rangle
\]

Interchanging \(\lambda^\prime\)'s and \(\lambda^{\prime\prime}\)'s,

\[
\langle\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}|L_1|\lambda_1^\prime\lambda_2^\prime...\lambda_u^\prime\rangle = \lambda_1^{\prime\prime}\langle\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}|\lambda_1^\prime\lambda_2^\prime...\lambda_u^\prime\rangle
\]

On account of the basic bras being orthogonal, the right-hand sides
here vanish unless \(\lambda_r^{\prime\prime} = \lambda_r^\prime\) for all \(r\) from 1 to \(u\), in which case the
right-hand sides are equal, and they are also real, \(\lambda_1^\prime\) being real. Thus,
whether the \(\lambda^{\prime\prime}\)'s are equal to the \(\lambda^\prime\)'s or not,

\[
\begin{align}
\langle\lambda_1^\prime\lambda_2^\prime...\lambda_i^\prime|L_1|\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}\rangle &= \overline{\langle\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}|L_1|\lambda_1^\prime\lambda_2^\prime...\lambda_u^\prime\rangle} \\
\\
&=\langle\lambda_1^\prime\lambda_2^\prime...\lambda_u^\prime|\overline{L}_1|\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}\rangle
\end{align}
\]

from equation (4) of §8. Since the \(\langle\lambda_1^\prime\lambda_2^\prime...\lambda_u^\prime|\)'s form a complete set
of bras and the \(|\lambda_1^{\prime\prime}\lambda_2^{\prime\prime}...\lambda_u^{\prime\prime}|a\rangle\)'s form a complete set of kets, we can infer that \(L_1 = \overline{L}_1\). The further condition required for \(L_1\) to be an
observable, namely that its eigenstates shall form a complete set, is
obviously satisfied since it has as eigenbras the basic bras, which
form a complete set.

</p><p>
We can similarly introduce linear operators \(L_2,L_3,...,L_u\) by multi-
plying \(\langle\lambda_1\lambda_2...\lambda_u|a\rangle\) by the factors \(\lambda_2,\lambda_3,...,\lambda_u\) in turn and considering
the resulting sets of numbers as representatives of kets. Each of these
\(L\)'s can be shown in the same way to have the basic bras as eigenbras
and to be real and an observable. The basic bras are simultaneous
eigenbras of all the \(L\)'s. Since these simultaneous eigenbras form a
complete set, it follows from a theorem of §13 that any two of the
\(L\)'s commute.

</p><p>
It will now be shown that, if \(\xi_1,\xi_2,...,\xi_u\) are any set of commuting
observables, we can set up an orthogonal representation in which the basic
bras are simultaneous eigenbras of \(\xi_1,\xi_2,...,\xi_u\). Let us suppose first that
there is only one independent simultaneous eigenbra of \(\xi_1,\xi_2,...,\xi_u\)
belonging to any set of eigenvalues \(\xi_1^\prime,\xi_2^\prime,...,\xi_u^\prime\). Then we may take
these simultaneous eigenbras, with arbitrary numerical coefficients, as
our basic bras. They are all orthogonal on account of the orthogonality
theorem (any two of them will have at least one eigenvalue different,
which is sufficient to make them orthogonal) and there are sufficient
of them to form a complete set, from a result of §13. They may
conveniently be labelled by the eigenvalues \(\xi_1^\prime,\xi_2^\prime,...,\xi_u^\prime\) to which they
belong, so that one of them is written \(\langle\xi_1^\prime\xi_2^\prime...\xi_u^\prime|\).

</p><p>
Passing now to the general case when there are several independent
simultaneous eigenbras of \(\xi_1,\xi_2,...,\xi_u\) belonging to some sets of eigen-
values, we must pick out from all the simultaneous eigenbras belong-
ing to a set of eigenvalues \(\xi_1^\prime,\xi_2^\prime,...,\xi_u^\prime\) a complete subset, the members
of which are all orthogonal to one another. (The condition of com-
pleteness here means that any simultaneous eigenbra belonging to the eigenvalues \(\xi_1^\prime,\xi_2^\prime,...,\xi_u^\prime\) can be expressed linearly in terms of the members of the subset.) We must do this for each set of eigenvalues
\(\xi_1^\prime,\xi_2^\prime,...,\xi_u^\prime\) and then put all the members of all the subsets together
and take them as the basic bras of the representation. These bras
are all orthogonal, two of them being orthogonal from the orthogona-
lity theorem if they belong to different sets of eigenvalues and from
the special way in which they were chosen if they belong to the same
set of eigenvalues, and they form altogether a complete set of bras,
as any bra can be expressed linearly in terms of simultaneous eigen-
bras and' each simultaneous eigenbra can then be expressed linearly
in terms of the members of a subset. There are infinitely many ways
of choosing the subsets, and each way provides one orthogonal
representation.

</p><p>
For labelling the basic bras in this general case, we may use the
eigenvalues \(\xi_1^\prime,\xi_2^\prime,...,\xi_u^\prime\) to which they belong, together with certain
additional real variables \(\lambda_1,\lambda_2,...,\lambda_v\) say, which must be introduced to
distinguish basic vectors belonging to the same set of eigenvalues
from one another. A basic bra is then written \(\langle\xi_1^\prime\xi_2^\prime...\xi_u^\prime\lambda_1\lambda_2...\lambda_v|\).
Corresponding to the variables \(\lambda_1,\lambda_2,...,\lambda_v\) we can define linear
operators \(L_1,L_2,...,L_v\) by equations like (1) and can show that these
linear operators have the basic bras as eigenbras, and that they are
real and observables, and that they commute with one another and
with the \(\xi\)'s. The basic bras are now simultaneous eigenbras of all
the commuting observables \(\xi_1,\xi_2,...,\xi_u,L_1,L_2,...,L_v\).

</p><p>
Let us define a complete set of commuting observables to be a set of
observables which all commute with one another and for which there
is only one simultaneous eigenstate belonging to any set of eigen-
values. Then the observables \(\xi_1,\xi_2,...,\xi_u,L_1,L_2,...,L_v\) form a complete
set of commuting observables, there being only one independent simul-
taneous eigenbra belonging to the eigenvalues \(\xi_1^\prime,\xi_2^\prime,...,\xi_u^\prime,\lambda_1,\lambda_2,...,\lambda_v\), namely the corresponding basic bra. Similarly the observables
\(L_1,L_2,...,L_u\) defined by equation (1) and the following work form
a complete set of commuting observables. With the help of this
definition the main results of the present section can be concisely
formulated thus:

(i) The basic bras of an orthogonal representation are simul-
taneous eigenbras of a complete set of commuting observ-
ables.

(ii) Given a complete set of commuting observables, we can set
up an orthogonal representation in which the basic bras are
simultaneous eigenbras of this complete set.

(iii) Any set of commuting observables can be made into a com-
plete commuting set by adding certain observables to it.

(iv) A convenient way of labelling the basic bras of an orthogonal
representation is by means of the eigenvalues of the complete
set of commuting observables of which the basic bras are
simultaneous eigenbras.

</p><p>
The conjugate imaginaries of the basic bras of a representation we
call the basic kets of the representation. Thus, if the basic bras are
denoted by \(\langle\lambda_1\lambda_2...\lambda_u|\), the basic kets will be denoted by \(|\lambda_1\lambda_2...\lambda_u\rangle\).
The representative of a bra \(\langle b|\) is given by its scalar product with
each of the basic kets, i.e. by \(\langle b|\lambda_1\lambda_2...\lambda_u\rangle\). It may, like the repre-
sentative of a ket, be looked upon either as a set of numbers or as a
function of the variables \(\lambda_1,\lambda_2,...,\lambda_u\). We have

\[
\langle b|\lambda_1\lambda_2...\lambda_u\rangle = \overline{\langle\lambda_1\lambda_2...\lambda_u|b\rangle}
\]

showing that the representative of a bra is the conjugate complex of the
representative of the conjugate imaginary ket. In an orthogonal repre-
sentation, where the basic bras are simultaneous eigenbras of a com-
plete set of commuting observables, \(\xi_1,\xi_2,...,\xi_u\) say, the basic kets
will be simultaneous eigenkets of \(\xi_1,\xi_2,...,\xi_u\).

</p><p>
We have not yet considered the lengths of the basic vectors. With
an orthogonal representation, the natural thing to do is to normalize
the basic vectors, rather than leave their lengths arbitrary, and so
introduce a further stage of simplification into the representation.
However, it is possible to normalize them only if the parameters
which label them all take on discrete values. If any of these para-
meters are continuous variables that can take on all values in a range,
the basic vectors are eigenvectors of some observable belonging to
eigenvalues in a range and are of infinite length, from the discussion
in §10 (see p.39 and top of p.40). Some-other procedure is then
needed to fix the numerical factors by which the basic vectors may
be multiplied. To get a convenient method of handling this question
anew mathematical notation is required, which will be given in the
next section.
</p><p>

<h3>15.The \(\delta\) function</h3>
<p>
Our work in §10 led us to consider quantities involving a certain

kind of infinity. To get a precise notation for dealing with these
infinities, we introduce a quantity \(\delta(x)\) depending on a parameter \(x\)
satisfying the conditions

\[
\left.
\begin{align}
\int_{-\infty}^\infty \delta(x)\;dx &= 1 \\
\\
\delta(x) &= 0\;for\; x \neq 0
\end{align}
\right\}
\tag{2}
\]

To get a picture of \(\delta(x)\), take a function of the real variable \(x\) which
vanishes everywhere except inside a small domain, of length \(\epsilon\) say,
surrounding the origin \(x = 0\), and which is so large inside this domain
. that its integral over this domain is unity. The exact shape of the
function inside this domain does not matter, provided there are no
unnecessarily wild variations (for example provided the function
is always of order \(\epsilon^{-1}\)). Then in the limit \(\epsilon \rightarrow 0\) this function will go
over into \(\delta(x)\).

</p><p>
\(\delta(x)\) is not a function of \(x\) according to the usual mathematical
definition of a function, which requires a function to have a definite
value for each point in its domain, but is something more general,
which we may call an ‘improper function' to show up its difference
from a function defined by the usual definition. Thus \(\delta(x)\) is not a
quantity which can be generally used in mathematical analysis like
an ordinary function, but its use must be confined to certain simple
types of expression for which it is obvious that no inconsistency
can arise.

</p><p>
The most important property of \(\delta(x)\) is exemplified by the follow-
ing equation,

\[
\int_{-\infty}^\infty f(x)\delta(x) dx=f(0) \tag{3}
\]

where \(f(x)\) is any continuous function of \(x\). We can easily see the
validity of this equation from the above picture of \(\delta(x)\). The left-
hand side of (3) can depend only on the values of \(f(x)\) very close
to the origin, so that we may replace \(f(x)\) by its value at the origin,
\(f(0)\), without essential error. Equation (3) then follows from the
first of equations (2). By making a change of origin in (3), we can
deduce the formula 

\[
\int_{-\infty}^\infty f(x)\delta(x-a)dx=f(a)  \tag{4}
\]

where \(a\) is any real number. Thus the process of multiplying a function
of \(x\) by \(\delta(x-a)\) and integrating over all \(x\) is equivalent to the process of
substituting \(a\) for \(x\). This general result holds also if the function of \(x\) is
not a numerical one, but is a vector or linear operator depending on \(x\).

</p><p>
The range of integration in (3) and (4) need not be from \(-\infty\) to \(\infty\),
but may be over any domain surrounding the critical point at which
the \(\delta\) function does not vanish. In future the limits of integration
will usually be omitted in such equations, it being understood that
the domain of integration is a suitable one.

</p><p>
Equations (3) and (4) show that, although an improper function
does not itself have a well-defined value, when it occurs as a factor
in an integrand the integral has a well-defined value. In quantum
theory, whenever an improper function appears, it will be something
which is to be used ultimately in an integrand. Therefore it should be
possible to rewrite the theory in a form in which the improper func-
tions appear all through only in integrands. One could then eliminate
the improper functions altogether. The use of improper functions
thus does not involve any lack of rigour in the theory, but is merely
a convenient notation, enabling us to express in a concise form
certain relations which we could, if necessary, rewrite in a form not
involving improper functions, but only in a cumbersome way which
would tend to obscure the argument.

</p><p>
An alternative way of defining the \(\delta\) function is as the differential
coefficient \(\epsilon^\prime(x)\) of the function \(\epsilon(x)\) given by

\[
\left.
\begin{align}
\epsilon(x) &= 0\; (x < 0) \\
\\
&= 1\; (x \gt 0)
\end{align}
\right\}
\tag{5}
\]

We may verify that this is equivalent to the previous definition by
substituting \(\epsilon^\prime(x)\) for \(\delta(x)\) in the left-hand side of (3) and integrating
by parts. We find, for \(g_1\) and \(g_2\) two positive numbers,

\[
\begin{align}
\int_{-g_2}^{g_1} f(x)\epsilon^\prime(x)dx &= \Big[f(x)\epsilon(x)\Big]_{-g_2}^{g_1}-\int_{-g_2}^{g_1} f^\prime(x)\epsilon(x)dx \\
\\
&= f(g_1) - \int_0^{g_1} f^\prime(x)dx \\
\\
&= f(0)
\end{align}
\]

in agreement with (3). The \(\delta\) function appears whenever one differen-
tiates a discontinuous function.

</p><p>
There are a number of elementary equations which one can write
down about \(\delta\) functions. These equations are essentially rules of
manipulation for algebraic work involving 5 functions. The meaning
of any of these equations is that its two sides give equivalent results
as factors in an integrand.

</p><p>
Examples of such equations are

\[
\begin{align}
\delta(-x) &= \delta(x) \tag{6} \\
\\
x\delta(x) &= 0  \tag{7} \\
\\
\delta(ax) &= a^{-1}\delta(x)\; (a > 0)  \tag{8} \\
\\
\delta(x^2—a^2) &= \frac{1}{2}a^{-1}\{\delta(x—a)+\delta(x+a)\}\; (a > 0)  \tag{9} \\
\\
\int \delta(a—x)\, dx\, \delta(x—b) &= \delta(a—b)  \tag{10} \\
\\
f(x)\delta(x—a) &= f(a)\delta(x—a)  \tag{11}
\end{align}
\]

Equation (6), which merely states that \(\delta(x)\) is an even function of its
variable \(x\) is trivial. To verify (7) take any continuous function of
\(x, f(x)\). Then

\[
\int f(x)x\delta(x)dx = 0
\]

from (3). Thus \(x\delta(x)\) as a factor in an integrand is equivalent to
zero, which is just the meaning of (7). (8) and (9) may be verified
by similar elementary arguments. To verify (10) take any continuous
function of \(a,f(a)\). Then

\[
\begin{align}
\int f(a)da \int \delta(a-x)dx\delta(x-b) &= \int \delta(x-b)dx\int f(a)da\delta(a-x) \\
\\
&= \int \delta(x-b)dx f(x) = \int f(a) da \delta(a-b)
\end{align}
\]

Thus the two sides of (10) are equivalent as factors in an integrand
with a as variable of integration. It may be shown in the same way
that they are equivalent also as factors in an integrand with 6 as
variable of integration, so that equation (10) is justified from either
of these points of view. Equation (11) is also easily justified, with
the help of (4), from two points of view.

</p><p>
Equation (10) would be given by an application of (4) with
f(x) = 8(v—b). We have here an illustration of the fact that we may
often use an improper function as though it were an ordinary con-
tinuous function, without getting a wrong result.

</p><p>
Equation (7) shows that, whenever one divides both sides of an
equation by a variable x which can take on the value zero, one
should add on to one side an arbitrary multiple of 5(x), ie. from an

equation A=B (12)
one cannot infer Aja = B/z,
but only Af = B/e+cd(x), (13)

where ¢ is unknown. ©

</p><p>
As an illustration of work with the 6 function, we may consider the
differentiation of log x. The usual formula

toga = - (14)
requires examination for the neighbourhood of z = 0. In order to
make the reciprocal function 1/z well defined in the neighbourhood
of « = 0 (in the sense of an improper function) we must impose on
it an extra condition, such as that its integral from —e to ¢ vanishes.
With this extra condition, the integral of the right-hand side of (14)
from —e to ¢ vanishes, while that of the left-hand side of (14) equals
log (—1), so that (14) is not a correct equation. To correct it, we must
remember that, taking principal values, loga has a pure imaginary
term iz for negative values of x. As x passes through the value zero
this pure imaginary term vanishes discontinuously. The differen-
tiation of this pure imaginary term gives us the result —i7 (x), so
that (14) should read

d 1,
ag oe = 5 in 8x). (15)

e

The particular combination of reciprocal function and 6 function
appearing in (15) plays an important part in the quantum theory of
collision processes (see § 50).
</p><p>

<h3>16.Properties of the basic vectors</h3>
<p>
Using the notation of the 6 function, we can proceed with the theory
of representations. Let us suppose first that we have a single observ-
able £ forming by itself a complete commuting set, the condition for
this being that there is only one eigenstate of € belonging to any
eigenvalue £', and let us set up an orthogonal representation in which
the basic vectors are eigenvectors of € and are written <£"|, |£‘>.

</p><p>
In the case when the eigenvalues of é are discrete, we can normalize
the basic vectors, and we then have

ElED =O EAE),

ele) = 1
These equations can be combined into the single equation
EE") = Beer, (16)

where the symbol 8 with two suffixes, which we shall often use in the
future, has the meaning

§,,= 0 when rs
= 1 when r=s.

} an

</p><p>
In the case when the eigenvalues of € are continuous we cannot

normalize the basic vectors. If we now consider the quantity ¢é"|€">”
with €' fixed and é” varying, we see from the work connected with
expression (29) of § 10 that this quantity vanishes for £” 4 é' and
that its integral over a range of é” extending through the value ¢'
is finite, equal to c say. Thus

CEE") = 0 8(E—2").
From (30) of § 10, ¢ is a positive number. It may vary with €', so
we should write it c(é') or c' for brevity, and thus we have

<E'|E") = 0 3(E— 8"). (18)
Alternatively, we have

E18") = 0 8(E'—8"), (19)
where c” is short for c(€é”), the right-hand sides of (18) and (19) being
equal on account of (11).

</p><p>
Let us pass to another representation whose basic vectors are
eigenvectors of £, the new basic vectors being numerical multiples of
the previous ones. Calling the new basic vectors <é'*|, |E'*>, with the
- additional label * to distinguish them from the previous ones, we have

E*[ = WEI, ED = BED,

where &' is short for &(é') and is a number depending on £'. We get
CEE — WENGE") = Mie! 5(e'—2")

with the help of (18). This may be written
(EME) = W'k'c' BE" —£")

from (11). By choosing &' so that its modulus is c'~?, which is possible
since c' is positive, we arrange to have

KEEN == O(E' — 6"). (20)

The lengths of the new basic vectors are now fixed so as to make the
representation as simple as possible. The way these lengths were
fixed is in some respects analogous to the normalizing of the basic
vectors in the case of discrete €', equation (20) being of the form of
(16) with the 3 function 6(¢'—€") replacing the 6 symbol dz, of
equation (16). We shall continue to work with the new representation
and shall drop the * labels in it to save writing. Thus (20) will now

be written <e/|é") = 8(€'—£"). (21)

</p><p>
We can develop the theory on closely parallel lines for the discrete
and continuous cases. For the discrete case we have, using (16),

2 IEEE") = 2 Edger = 12",

the sum being taken over all eigenvalues. This equation holds for
any basic ket |£"> and hence, since the basic kets form a complete set,

2 Ie><o"] = 1 (22)
This is a useful equation expressing an important property of the
basic vectors, namely, of [£'> is multiplied on the right by <é'| the
resulting linear operator, summed for all &', equals the unit operator.
Equations (16) and (22) give the fundamental properties of the basic

vectors for the discrete case.

</p><p>
Similarly, for the continuous case we have, using (21),

| Ig") df' EE) = | If") dg' (g'—&") = |£" (23)

from (4) applied with a ket vector for f(x), the range of integration
being the range of eigenvalues. This holds for any basic ket |£”)>

and hence
flé>ae ei =1. (24)
﻿64 REPRESENTATIONS § 16

This is of the same form as (22) with an integral replacing the sum.
Equations (21) and (24) give the fundamental properties of the basic
vectors for the continuous case.

</p><p>
Equations (22) and (24) enable one to expand any bra or ket in
terms of the basic vectors. For example, we get for the ket |P> in the
discrete case, by multiplying (22) on the right by |P>,

[P> = 2 le><E|P>, (25)

which gives |P> expanded in terms of the |£'>'s and shows that the
coefficients in the expansion are <£'|P)>, which are just the numbers
forming the representative of |P)>. Similarly, in the continuous case,

[P> = { le' de" <€'1P>, (26)

giving |P) as an integral over the |£">'s, with the coefficient in the
integrand again just the representative <¢'|P) of |P>. The conjugate
imaginary equations to (25) and (26) would give the bra vector <P|
expanded in terms of the basic bras.

Our present mathematical methods enable us in the continuous
case to expand any ket as an integral of eigenkets of £. If we do not
use the 8 function notation, the expansion ofa general ket will consist
of an integral plus a sum, as in equation (25) of § 10, but the 6 function
enables us to replace the sum by an integral in which the integrand
consists of terms each containing a 6 function as a factor. For
example, the eigenket |¢”> may be replaced by an integral of eigen-
kets, as is shown by the second of equations (23).

If (Q| is any bra and |P> any ket we get, by further applications

&

for discrete é' and

<QIP> = [ <Qle' dé" <e|P> (28)

for continuous £'. These equations express the scalar product of (Q|
and |P» in terms of their representatives (Q|é'> and <é"|P>. Equa-
tion (27) is just the usual formula for the scalar product of two
vectors in terms of the coordinates of the vectors, and (28) is the
natural modification of this formula for the case of continuous &',
with an integral instead of a sum.

The generalization of the foregoing work to the case when é has
both discrete and continuous eigenvalues is quite straightforward.
﻿§ 16 PROPERTIES OF THE BASIC VECTORS 65

Using & and & to denote discrete eigenvalues and ¢' and £” to denote
continuous eigenvalues, we have the set of equations

KEE = See, EE D = 0, EE") = OEE") (29)
as the generalization of (16) or (21). These equations express that
the basic vectors are all orthogonal, that those belonging to discrete
eigenvalues are normalized and those belonging to continuous eigen-
values have their lengths fixed by the same rule as led to (20). From
(29) we can derive, as the generalization of (22) or (24),

FOE L+ [eae l= 1 (30)

the range of integration being the range of continuous eigenvalues.
With the help of (30), we get immediately

IP> = Ble <elPy+ J |e> dé' <e'1P> (31)
as the generalization of (25) or (26), and
COIP) = 3 CQL IP>-+ f <@lg> de' <e'1P> (82)

as the generalization of (27) or (28).

' Let us now pass to the general case when we have several commuting
observables &,, &,,..., €,, forming a complete commuting set and set up
an orthogonal representation in which the basic vectors are simul-
taneous eigenvectors of all of them, and are written <&...€)|, |&..-&)-
Let us suppose §,,&,...,€, (v <u) have discrete eigenvalues and
Evite» &y, have continuous eigenvalues.

Consider the quantity <£)..6,6541.-4416.-6) 6541-62. From the
orthogonality theorem, it must vanish unless each £3 = & for
s=v-+l,..,u. By extending the work connected with expression
(29) of §10 to simultaneous eigenvectors of several commuting
observables and extending also the axiom (30), we find that the
(u—v)-fold integral of this quantity with respect to each & over
@ range extending through the value €, is a finite positive number.
Calling this number c', the ' denoting that it is a function of
E500) os Costes Ses WE CaN express our results by the equation

<E4--b0 Sosa Slot So Sos Sar = 6 8(Sosr— S41) S(Eu— Ea)» (33)

with one 8 factor on the right-hand side for each value of s from
v-+1 tou. We now change the lengths of our basic vectors so as to
﻿66 REPRESENTATIONS § 16

make c' unity, by a procedure similar to that which led to (20). By
a further use of the orthogonality theorem, we get finally

Cie bul€i---Sa> = beer 86 8(So41— Soar) O(a &), (84)

~

with a two-suffix 6 symbol on the rigut-hand side for each £ with
discrete eigenvalues and a 8 function for each € with continuous
eigenvalues. This is the generalization of (16) or (21) to the case when
there are several commuting observables in the complete set.

From (34) we can derive, as the generalization of (22) or (24)

XS fof Gd dea dl Gal = 1 (35)

the integral being a (u-—v)-fold one over all the és with continuous
eigenvalues and the summation being over all the €'s with discrete
eigenvalues. Equations (34) and (35) give the fundamental properties
of the basic vectors in the present case. From (35) we can imme-
diately write down the generalization of (25) or (26) and of (27) or (28).

The case we have just considered can be further generalized by
allowing some of the é's to have both discrete and continuous eigen-
values. The modifications required in the equations are quite straight-
forward, but will not be given here as they are rather cumbersome to
write down in general form.

There are some problems in which it is convenient not to make the
c' of equation (33) equal unity, but to make it equal to some definite
function of the £''s instead. Calling this function of the €'s p'~1 we
then have, instead of (34)

(ba bl ad = pS er Seces 8(Sora Eoaa)8(Eu— Su), (38)
and instead of (35) we get

Sfp kd 0” ear de, Ei bul = Le (37)

p' is called the weight function of the representation, p' df,,,..d&,
being the ‘weight' attached to a small volume element of the space
of the variables &,,1,.., &,

The representations we considered previously all had the weight
function unity. The introduction of a weight function not unity is
entirely a matter of convenience and does not add anything to the
mathematical power of the representation. The basic bras (...),*|
of a representation with the weight function p' are connected with
﻿§ 16 PROPERTIES OF THE BASIC VECTORS 67

the basic bras <&,...¢;,| of the corresponding representation with the
weight function unity by

(Ea bu] = pK EL Sul, (38)
as is easily verified. An example of a useful representation with
non-unit weight function occurs when one has two é's which are
the polar and azimuthal angles 6 and ¢ giving a direction in three-
dimensional space and one takes p' = sin 6'. One then has the element
of solid angle sin @' d6'dé' occurring in (37).

17. The representation of linear operators

In § 14 we saw how to represent ket and bra vectors by sets of
numbers. We now have to do the same for linear operators, in order
to have a complete scheme for representing all our abstract quantities
by sets of numbers. The same basic vectors that we had in § 14 can
be used again for this purpose.

Let us suppose the basic vectors are simultaneous eigenvectors of
a complete set of commuting observables €,,é,...,¢,. If a is any
linear operator, we take a general basic bra <&...€,/ and a general
basic ket |€]...6,> and form the numbers

(Ey. Eglalel..€2>. (39)
These numbers are sufficient to determine a completely, since in the
' first place they determine the ket «|&...€%> (as they provide the
representative of this ket), and the value of this ket for all the basic
kets [£%...€”) determines a. The numbers (39) are called the repre-
sentative of the linear operator « or of the dynamical variable «. They
are more complicated than the representative of a ket or bra vector
in that they involve the parameters that label two basic vectors
instead of one.

Let us examine the form of these numbers in simple cases. Take
first the case when there is only one £, forming a complete commuting
set by itself, and suppose that it has discrete eigenvalues ¢'. The
representative of « is then the discrete set of numbers <é'|alé”>. If
one had to write out these numbers explicitly, the natural way of
arranging them would be as a two-dimensional array, thus:

Erol  <EtalE*™> — <S*] ax E>
Cole EPSP) <P lal E>
EPloelE> <ePlwlE> << HalE> (40)
﻿68 REPRESENTATIONS §17

where £1, £, &,.. are all the eigenvalues of €&. Such an array is called
a matrix and the numbers are called the clements of the matrix. We
make the convention that the elements must always be arranged so
that those in the same row refer to the same basic bra vector and
those in the same column refer to the same basic ket vector.

An element <¢'la|£'> referring to two basic vectors with the same
label is called a diagonal element of the matrix, as all such elements
lie on a diagonal. If we put « equal to unity, we have from (16) ail
the diagonal elements equal to unity and all the other elements equal
to zero. The matrix is then called the unit matrix.

If « is real, we have

CE" lol") = <e" lal E>. (41)

The effect of these conditions on the matrix (40) is to make the

diagonal elements all real and each of the other elements equal the

conjugate complex ofits mirror reflection in the diagonal. ‘The matrix
is then called a Hermitian matrix.

If we put « equal to &, we get for a general element of the matrix

CE EE = ENED = EF Oper. (42)
Thus all the elements not on the diagonal are zero. The matrix is
then called a diagonal matriz. Its diagonal elements are just equal

to the eigenvalues of €. More generally, if we put a equal to f(£), a
function of £, we get

ETF ENED = LE) Bee, (43)
and the matrix is again a diagonal matrix.
Let us determine the representative of a product af of two linear

operators a and f in terms of the representatives of the factors.
From equation (22) with £” substituted for é' we obtain

= » CE" JoelE”><o" BIE", (44)

which gives us the required result. Equation (44) shows that the
matrix formed by the elements <£'|o8|£”> equals the product of the
matrices formed by the elements <£'|«|£”> and <é'|B|£"> respectively,
according to the usual mathematical rule for multiplying matrices.
This rule gives for the element in the rth row and sth column of the
product matrix the sum of the product of each element in the rth
row of the first factor matrix with the corresponding element in the sth
﻿$17 THE REPRESENTATION OF LINEAR OPERATORS 69

column of the second factor matrix. The multiplication of matrices
is non-commutative, like the multiplication of linear operators.

We can summarize our results for the case when there is only one
é and it has discrete eigenvalues as follows:

(i) Any linear operator is represented by a matrix.

(ii) The unit operator is represented by the umt matrix.

(iii) A real linear operator is represented by a Hermitian matrix.
(iv) € and functions of & are represented by diagonal matrices.

(v) The mairiz representing the product of two linear operators 1s the
product of the matrices representing the two factors.

Let us now consider the case when there is only one é and it has
continuous eigenvalues. The representative of « is now <é"|a|é, a
function of two variables £ and é” which can vary continuously. It
is convenient to call such a function a ‘matrix', using this word in
a generalized sense, in order that we may be able to use the same
terminology for the discrete and continuous cases. One of these
generalized matrices cannot, of course, be written out as a two-
dimensional array like an ordinary matrix, since the number of its
rows and columns is an infinity equal to the number of points on a
line, and the number of its elements is an infinity equal to the

number of points in an area.
We arrange our definitions concerning these generalized matrices

so that the rules (i)-(v) which we had above for the discrete case
hold also for the continuous case. The unit operator is represented
by 6(é'—&') and the generalized matrix formed by these elements
we define to be the unit matrix. We still have equation (41) as the
condition for a to be real and we define the generalized matrix formed
by the elements <&'|a|é"> to be Hermitian when it satisfies this
condition. € is represented by

Ce'e|E"> = £'8('—£") (45)
and f(£) by SAIED = FEV 5(E—€"), (46)
and the generalized matrices formed by these elements we define to be
diagonal matrices. From (11), we could equally well have é" and f(é”)
as the coefficients of 8(é'—€”) on the right-hand sides of (45) and (46)
respectively. Corresponding to equation (44) we now have, from (24)

é'lople"> = [ <é'lale”> dé” <E"iBIE">, (47)

with an integral instead of a sum, and we define the generalized

matrix formed by the elements on the right-hand side here to be the
 3695,57 a)
﻿70 REPRESENTATIONS §17

product of the matrices formed by <€'Ja/é”> and <é'|B{Eé">. With
these definitions we secure complete parallelism between the discrete
and continuous cases and we have the rules (i)-(v) holding for both.

The question arises how a general diagonal matrix is to be defined
in the continuous case, as so far we have only defined the right-hand
sides of (45) and (46) to be examples of diagonal matrices. One
might.be inclined to define as diagonal any matrix whose (€', €”)
elements all vanish except when €' differs infinitely little from &',
but this would not be satisfactory, because an important property
of diagonal matrices in the discrete case is that they always commute
with one another and we want this property to hold also in the
continuous case. In order that the matrix formed by the elements ~
<€'|w|é") in the continuous case may commute with that formed by
the elements on the right-hand side of (45) we must have, using the
multiplication rule (47),

i <e' jw \e"> ag” £78 (€"—£") = | ' &(E —&") dé” cE" lw |é">.
With the help of formula (4), this reduces to
CE OEE” = ECE ew |E" (48)

This gives, according to the rule by which (13) follows from (12),

CE |wig"> = 0 B(E' —€")

where c' is a number that may depend on ¢'. Thus <€'|w/E”) is of the
form of the right-hand side of (46). For this reason we define only
matrices whose elements are of the form of the right-hand side of (46) to
be diagonal mairices. It is easily verified that these matrices all
commute with one another. One can form other matrices whose
(&', €”) elements all vanish when €' differs appreciably from £” and
have a different form of singularity when é' equals £” [we shall later
introduce the derivative 8'(x) of the 5 function and 8'(é'—&”) will
then be an example, see § 22 equation (19)], but these other matrices
are not diagonal according to the definition.

Let us now pass on to the case when there is only one £ and it has
both discrete and continuous eigenvalues. Using é7,& to denote
discrete eigenvalues and €',é” to denote continuous eigenvalues, we
now have the representative of « consisting of four kinds of quanti-

ties, <E7la|E >, (E"la |, <E"lalE, <Efalé">. These quantities can all
﻿§17 THE REPRESENTATION OF LINEAR OPERATORS 71

be put together and considered to form a more general kind of matrix
having some discrete rows and columns and also a continuous range
of rows and columns. We define unit matrix, Hermitian matrix,
diagonal matrix, and the product of two matrices also for this more
general kind of matrix so as to make the rules (i)-(v) still hold. The
details are a straightforward generalization of what has gone before
and need not be given explicitly.

Let us now go back to the general case of several é's, &,, &,...,&,.
The representative of «, expression (39), may still be looked upon as
forming a matrix, with rows corresponding to different values of
&,..,&, and columns corresponding to different values of &...., &%.
Unless ail the £'s have discrete eigenvalues, this matrix will be of the
generalized kind with continuous ranges of rows and columns. We
again arrange our definitions so that the rules (i)-(v) hold, with rule
(iv) generalized to:

(iv') Hach &,, (m = 1,2,...,u) and any function of them is repre-
sented by a diagonal matrix.

A diagonal matrix is now defined as one whose general element
(€;.-€,/al€]...€2> is of the form

(bh Sloe Ftd = 6" 84, ¢1--Be, 223 (ou— Sou) H(E,— Ea) (49)

in the case when £,,..,&, have discrete eigenvalues and é,.,,..,€,, have
continuous eigenvalues, c' being any function of the £'s. This defini-
tion is the generalization of what we had with one é and makes
diagonal matrices always commute with one another. The other
definitions are straightforward and need not be given explicitly.

* We now have a linear operator always represented by a matrix.
The sum of two linear operators is represented by the sum of the
matrices representing the operators and this, together with rule (v),
means that the matrices are subject to the same algebraic relations as
the linear operators. If any algebraic equation holds between certain
linear operators, the same equation must hold between the matrices
representing those operators.

The scheme of matrices can be extended to bring in the repre-
sentatives of ket and bra vectors. The matrices representing linear
operators are all square matrices with the same number of rows and
columns, and with, in fact, a one-one correspondence between their
rows and columns. We may look upon the representative of a ket
|P> as a matrix with a single column by setting all the numbers
﻿72 REPRESENTATIONS §17

<é..€4|P> which form this representative one below the other. The
number of rows in this matrix will be the same as the number of
rows or columns in the square matrices representing linear operators.
Such a single-column matrix can be multiplied on the left by a square
matrix ¢&...€,|o|€]..¢2> representing a linear operator, by a rule
similar to that for the multiplication of two square matrices. The
product is another single-column matrix with elements given by

Dede] Ge Gulall-BO dia dE ESP).

From (35) this is just equal to <&...€,|«|P>, the. representative of
«|P>. Similarly we may look upon the representative of a bra ¢Q|
as a matrix with a single row by setting all the numbers <Q|£)...6)
side by side. Such a single-row matrix may be multiplied on the
right by a square matrix <£)...¢,,/a/£7...4>, the product being another
single-row matrix, which is just the representative of <Qla. The
single-row matrix representing <Q| may be multiplied on the right
by the single-column matrix representing |P>, the product being a
matrix with just a single element, which is equal to <Q|P>. Finally,
the single-row matrix representing (Q| may be multiplied on the left
by the single-column matrix representing |P>, the product being a
square matrix, which is just the representative of |P><@Q|. In this
way all our abstract symbols, linear operators, bra vectors, and ket
vectors, can be represented by matrices, which are subject to the
same algebraic relations as the abstract symbols themselves.

18. Probability amplitudes

Representations are of great importance in the physical interpreta-
tion of quantum mechanics as they provide a convenient method for
obtaining the probabilities of observables having given values. In
§ 12 we obtained the probability of an observable having any speci-
fied value for a given state and in § 13 we generalized this result
and obtained the probability of a set of commuting observables
simultaneously having specified values for a given state. Let us now
apply this result to a complete set of commuting observables, say the
set of ¢'s which we have been dealing with already. According to
formula (51) of § 13, the probability of each &, having the value &
for the state corresponding to the normalized ket vector |x) is

Pre, = St |8 eg, 5g, 5---8e,e,10- (50)
﻿§18 PROBABILITY AMPLITUDES 73

If the ¢'s all have discrete eigenvalues, we can use (35) with v = u
and no integrals, and get

Pee, = 2, <x |8¢, 2, 82, 250-5246, 184--Su> Sh Sar
Lead gg

= 2, Cac |Ser es Sez es. See en 1ST St <r Se. /@

= (elf Sd Sr Sul2>
= |6r--ule>?. (51)

We thus get the simple result that the probability of the €'s having the
values €' is just the square of the modulus of the appropriate coordinate
of the normalized ket vector corresponding to the state concerned.

Tf the é's do not all have discrete eigenvalues, but if, say, &,.., ,
have discrete eigenvalues and &,,1,..,€, have continuous eigenvalues,
then to get something physically significant we must obtain the
probability of each , (r = 1,..,v) having a specified value ¢, and each
é, (s = v-+1,..,u) lying in a specified small range & to &+dé,. For
this purpose we must replace each factor 8, ¢, in (50) by a factor x;,
which is that function of the observable ¢, which is equal to unity
for £, within the range &, to £,+-d£, and zero otherwise. Proceeding
as before with the help of (35), we obtain for this probability .

Pee dear dE, = 1G Sule>? doar dba (52)

Thus in every case the probability distribution of values for the €s ts
given by the square of the modulus of the representative of the norma-
lized ket vector corresponding to the state concerned.

The numbers which form the representative of a normalized ket
(or bra) may for this reason be called probability amplitudes. ‘The
square of the modulus of a probability amplitude is an ordinary
probability, or a probability per unit range for those variables that
have continuous ranges of values.

We may be interested in a state whose corresponding ket |x> cannot
be normalized. This occurs, for example, if the state is an eigenstate
of some observable belonging to an eigenvalue lying in a range of
eigenvalues. The formula (51) or (52) can then still be used to give
the relative probability of the é's having specified values or having
values lying in specified small ranges, i.e. it will give correctly the
ratios of the probabilities for different £'s. The numbers <&...&,|%>
may then be called relative probability amplitudes.
﻿74 REPRESENTATIONS § 18

The representation for which the above results hold is characterized
by the basic vectors being simultaneous eigenvectors of all the é's.
It may also be characterized by the requirement that each of the é's
shall be represented by a diagonal matrix, this condition being easily
seen to be equivalent to the previous one. The latter characterization
is usually the more convenient one. For brevity, we shall formulate
it as each of the é's “being diagonal in the representation'.

Provided the é's form a complete set of commuting observables,
the representation is completely determined by the characterization,
apart from arbitrary phase factors in the basic vectors. Each basic bra
<&...€,| may be multiplied by e%”', where y' is any real function of
the variables ¢},..., &, without changing any of the conditions which
the representation has to satisfy, i.c. the condition that the é's are
diagonal or that the basic vectors are simultaneous eigenvectors of
the £'s, and the fundamental properties of the basic vectors (34) and
(35). With the basic bras changed in this way, the representative
(&...€,|P> of a ket |P> gets multiplied by e*', the representative
(Q1g...€,> of a bra <Q} gets multiplied by e~%”' and the representa-
tive <&)...€,,|a|&]...6> of a linear operator « gets multiplied by e'-7”.
The probabilities or relative probabilities (51), (52) are, of course,
unaltered.

The probabilities that one calculates in practical problems in
quantum mechanics are nearly always obtained from the squares
of the moduli of probability amplitudes or relative probability ampli-
tudes. Even when one is interested only in the probability of an
incomplete set of commuting observables having specified values, it
is usually necessary first to make the set a complete one by the
introduction of some extra commuting observables and to obtain
the probability of the complete set having specified values (as the
square of the modulus of a probability amplitude), and then to sum
or integrate over all possible values of the extra observables. A
more direct application of formula (51) of §13 is usually not
practicable.

To introduce a representation in practice

(i) We look for observables which we would like to have diagonal,
either because we are interested in their probabilities or for
reasons of mathematical simplicity ;

(ii) We must see that they all commute—a necessary condition
since diagonal matrices always commute;
﻿§ 18 PROBABILITY AMPLITUDES 715

(iii) We then see that they form a complete commuting set, and
if not we add some more commuting observables to them to
make them into a complete commuting set;

(iv) We set up an orthogonal representation with this complete
commuting set diagonal.

The representation is then completely determined except for the
arbitrary phase factors. For most purposes the arbitrary phase
factors are unimportant and trivial, so that we may count the
representation as being completely determined by the observables
that are diagonal in it. This fact is already implied in our notation,
since the only indication in a representative of the representation to
which it belongs are the letters denoting the observables that are
diagonal.

It may be that we are interested in two representations for the
same dynamical system. Suppose that in one of them the complete
set of commuting observables &,,...,é, are diagonal and the basic
bras are ¢€;...€,| and in the other the complete set of commuting
observables 7,...,7,, are diagonal and the basic bras are <7}...7%)|-
A ket |P> will now have the two representatives <&...&,|P> and
<n Mw|P>. Tf &,..,é, have discrete eigenvalues and &,,,,.., é,, have
continuous eigenvalues and if 7,,..,7, have discrete eigenvalues and
Ne+19+ Ny have continuous eigenvalues, we get from (35)

Theol) = B fof titel fa> Board EnkulP>, (58)
and interchanging é's and 7's
(y--ul P> =>) Si Ealt te? Insane (Ii--Mw|P>. (54)

These are the transformation equations which give one representative
- of |P> in terms of the other. They show that either representative
is expressible linearly in terms of the other, with the quantities

Cm ule Eo, <n lam (55)

as coefficients. These quantities are called the transformation func-
tions. Similar equations may be written down to connect the two
representatives of a bra vector or of a linear operator. The trans-
formation functions (55) are in every case the means which enable
one to pass from one representative to the other. Each of the
﻿76 REPRESENTATIONS § 18

transformation functions is the conjugate complex of the other, and
they satisfy the conditions

x, fof <t--ntol fad Moar db, Er Gilttte to?

= Sint Oyen (Meta Mesa) No— Mo) (58)
and the corresponding conditions with é's and 7's interchanged, as
may be verified from (35) and (34) and the corresponding equations
for the 7's.

Transformation functions are examples of probability amplitudes
or relative probability amplitudes. Let us take the case when all the
é's and all the y's have discrete eigenvalues. Then the basic ket
|y4---7}y> is normalized, so that its representative in the €-representa-
tion, <€)...£4,|71---Mp>, is a probability amplitude for each set of values
for the £'s. The state to which these probability amplitudes refer,
namely the state corresponding to |7}...71,>, is characterized by the
condition that a simultaneous measurement of 7,,...,7,, is certain to
lead to the results 7j,...,M%- Thus [<€)...€,|71.--mo> ? is the proba-
bility of the é's having the values ¢...¢;, for the state for which the
n's certainly have the values 7}...7,,. Since

So Seel me me? = [om MlStSw Ps
we have the theorem of reciprocity—the probability of the és having
the values £' for the state for which the n's certainly have the values 1'
is equal to the probability of the 's having the values x' for the state for
which the &s certainly have the values £'. :

If all the 7's have discrete eigenvalues and some of the é's have
continuous eigenvalues, |<&;...,,|71---7,> |? still gives the probability
distribution of values for the é's for the state for which the »'s cer-
tainly have the values y'. If some of the y's have continuous eigen-
values, |7}...7,,> is not normalized and [(&...d,|y,-..4y>[? then gives
only the relative probability distribution of values for the é's for the
state for which the y's certainly have the values 7'.

19. Theorems about functions of observables

We shall illustrate the mathematical value of representations by
using them to prove some theorems.

THEOREM 1, A linear operator that commutes with an observable €
commutes also with any function of €.

The theorem is obviously true when the function is expressible as
﻿§19 THEOREMS ABOUT FUNCTIONS OF OBSERVABLES 77

a power series. To prove it generally, let w be the linear operator,
so that we have the equation

fw—wk = 0. (57)

Let us introduce a representation in which é is diagonal. If € by
itself does not form a complete commuting set of observables, we must
make it into a complete commuting set by adding certain observables,
B say, to it, and then take the representation in which € and the f's
are diagonal. (The case when £ does form a complete commuting set
by itself can be looked upon as a special case of the preceding one
with the number of f variables zero.) In this representation equation

(57) becomes <E'B" geo —wwé|£"B") = 0,
which reduces to
EB a | E"B") — CEB' ew |E"BE” = 0.
In the case when the eigenvalues of € are discrete, this equation
shows that all the matrix elements <&''|w|é”B"> of w vanish except

those for which ¢' = €”. In the case when the eigenvalues of & are
continuous it shows, like equation (48), that <é'B'[w|é"B”> is of the

form EB lole"B"> = o5(e'—£"),

where c is some function of €' and the §''s and 6''s. In either case
we may say that the matrix representing w ‘is diagonal with respect
to é'. Tf f(é) denotes any function of ¢ in accordance with the general
theory of § 11, which requires f(£”) to be defined for £” any eigenvalue
of £, we can deduce in either case

FEVER 021 E"B — CEB eo |E"B FE") = 0.
This gives CEB P(E) o—wf(E) EB = 0,
so that f()e—af(é) = 0

and the theorem is proved.

As a special case of the theorem, we have the result that any
observable that commutes with an observable € also commutes with
any function of €. This result appears as a physical necessity when
we identify, as in §13, the condition of commutability of two
observables with the condition of compatibility of the correspond-
ing observations. Any observation that is compatible with the
measurement of an observable € must also be compatible with the
measurement of f(é), since any measurement of & includes in itself
a measurement of /(é).
﻿78 REPRESENTATIONS § 19

TuHroreM 2. A linear operator that commutes with each of a complete
set of commuting observables is a function of those observables.

Let w be the linear operator and ¢,,&,...,6, the complete set of
commuting observables, and set up a representation with these
observables diagonal. Since w commutes with each of the &'s, the
matrix representing it is diagonal with respect to each of the £'s,
by the argument we had above. This matrix is therefore a diagonal
matrix and is of the form (49), involving a number c' which is a
function of the £'s. It thus represents the function of the £'s that
c' is of the é”s, and hence w equals this function of the €'s.

THEOREM 3. If an observable € and a linear operator g are such that
any linear operator that commutes with € also commutes with g, then g
ais a function of €.

This is the converse of Theorem 1. To prove it, we use the same
representation with ¢ diagonal as we had for Theorem 1. In the first
place, we see that g must commute with € itself, and hence the
representative of g must be diagonal with respect to €, i.e. it must
be of the form

<EB'IGIE'B'> = a(E'B'B' Bee or a(€'B'B")3(E'—&"),
according to whether & has discrete or continuous eigenvalues. Now
let w be any linear operator that commutes with €, so that its
representative is of the form
EB lwle”B"> = BEBB Beg or B(E'B'B")3E'—2").
By hypothesis w» must also commute with g, so that
<E'B' gu —eog |E"B"> = 0. (58)
If we suppose for definiteness that the f's have discrete eigenvalues,
(58) leads, with the help of the law of matrix multiplication, to
> {ale'B'B")O(E'B"B") — B(E'B'B" al E'B"B')} = 0, (59)

per
the left-hand side of (58) being equal to the left-hand side of (59)

multiplied by 8¢2 or 5(¢'—§"). Equation (59) must hold for all
functions b(é'B'B"). We can deduce that

a(g'B'B") = 0 for Bp' # B",
ate' p'p') = ale'B"B").
The first of these results shows that the matrix representing g is

diagonal and the second shows that a(é'B'f') is a function of €' only.
We can now infer that g is that function of € which a(é'B'f') is of €,
﻿$19 THEOREMS ABOUT FUNCTIONS OF OBSERVABLES 79

so the theorem is proved. The proof is analogous if some of the §'s

have continuous eigenvalues.
Theorems I and 3 are still valid if we replace the observable € by

any set of commuting observables €,, €,,..,€, only formal changes
being needed in the proofs.

20. Developments in notation

The theory of representations that we have developed provides a
general system for labelling kets and bras. Ina representation in which
the complete set of commuting observables &,,...,&, are diagonal any
ket | P > will have a representative <&...6,|P>, or (é'|P> for brevity.
This representative is a definite function of the variables £', say (£').
The function % then determines the ket |P> completely, so it may be
used to label this ket, to replace the arbitrary label P. In symbols,

if a
we put |P> = [(€)>.
We must put |P> equal to [b(é)> and not [b(é')>, since it does not

depend on a particular set of eigenvalues for the £'s, but only on the

form of the function y.
With f(é) any function of the observables &,...,&,, f(6)|P> will
have as its representative

CEE PD = FEVWME').
Thus according to (60) we put

FS)P> = FOYE).
With the help of the second of equations (60) we now get

FE)NEE) = FEME)>.- (61)

This is a general result holding for any functions f and ¢ of the s,
and it shows that the vertical line | is not necessary with the new
notation for a ket—either side of (61) may be written simply as
f(£)4(6)>. Thus the rule for the new notation becomes :—
we put [P> = $(é)>.
We may further shorten %(£)> to 4, leaving the variables ¢ under-
stood, if no ambiguity arises thereby.

The ket %(£)> may be considered as the product of the linear
operator %(¢) with a ket which is denoted simply by > without a
label. We call the ket > the standard ket. Any ket whatever can be
﻿80 REPRESENTATIONS § 20

expressed as a function of the é's multiplied into the standard ket.
For example, taking |P) in (62) to be the basic ket |¢">, we find

1E”> == 84, ¢7--Se,¢7 5(Eps1— Soa) S(Eu— Su)? (63)

in the case when ¢,,.., €, have discrete eigenvalues and €,,1,..,&,, have
continuous eigenvalues. The standard ket is characterized by the
condition that its representative <é'|> is unity over the whole domain
of the variable ¢', as may be seen by putting = 1 in (62).

A further contraction may be made in the notation, namely to
leave the symbol > for the standard ket understood. A ket is then
written simply as ¥4(é), a function of the observables €. A function
of the é's used in this way to denote a ket is called a wave function.}
The system of notation provided by wave functions is the one usually
used by most authors for calculations in quantum mechanics. In
using it one should remember that each wave function is understood
to have the standard ket multiplied into it on the right, which
prevents one from multiplying the wave function by any operator
on the right. Wave functions can be multiplied by operators only on
the left. This distinguishes them from ordinary functions of the &'s,
which are operators and can be multiplied by operators on either the
left or the right. A wave function is just the representative of a ket
expressed as a function of the observables é, instead of eigenvalues &'
for those observables. The square of its modulus gives the proba-
bility (or the relative probability, if it is not normalized) of the és
having specified values, or lying in specified small ranges, for the
corresponding state.

The new notation for bras may be developed in the same way as
for kets. A bra <Q| whose representative <Q|é'> is o(&') we write
<¢(€)|. With this notation the conjugate imaginary to |¢(£)> is
<b(€)|. Thus the rule that we have used hitherto, that a ket and
its conjugate imaginary bra are both specified by the same label,
must be extended to read—if the labels of a ket involve complex
numbers or complex functions, the labels of the conjugate imaginary
bra involve the conjugate complex numbers or functions. As in the
case of kets we can show that <4(€)|f(€) and <(é)f(é)| are the same,
so that the vertical line can be omitted. We can consider <(£) as
the product of the linear operator ¢(£) into the standard bra <, which

} The reason for this name is that in the early days of quantum mechanics all the

examples of these functions were of the form of waves. The name is not a descriptive
one from the point of view of the modern general theory.
﻿§ 20 DEVELOPMENTS IN NOTATION 81

is the conjugate imaginary of the standard ket >. We may leave
the standard bra understood, so that a general bra is written as $(€),
the conjugate complex of a wave function. The conjugate complex
of a wave function can be multiplied by any linear operator on the
right, but cannot be multiplied by a linear operator on the left. We
can construct triple products of the form <f(€)>. Such a triple product
is a number, equal to f(€) summed or integrated over the whole
domain of eigenvalues for the €'s,

fe = ff ME) Bor db (64)

in the case when €,,.., €, have discrete eigenvalues and €,,,,...,£, have
continuous eigenvalues.

The standard ket and bra are defined with respect to a representa-
tion. If we carried through the above work with a different repre-
sentation in which the complete set of commuting observables 7 are
diagonal, or if we merely changed the phase factors in the representa-
tion with the £'s diagonal, we should get a different standard ket and
bra. In a piece of work in which more than one standard ket or bra
appears one must, of course, distinguish them by giving them labels.

A further development of the notation which is of great importance
for dealing with complicated dynamical systems will now be discussed.
Suppose we have a dynamical system describable in terms of dynami-
cal variables which can all be divided into two sets, set A and set B
say, such that any member of set A commutes with any member of
set B. A general dynamical variable must be expressible as a function
of the A-variables and B-variables together. We may consider
another dynamical system in which the dynamical variables are the
A-variables only—let us call it the A-system. Similarly we may
consider a third dynamical system in which the dynamical variables
are the B-variables only—the B-system. The original system can
then be looked upon as a combination of the A-system and the
B-system in accordance with the mathematical scheme given below.

Let us take any ket |a> for the A-system and any ket |b> for the
B-system. We assume that they have a product |a>|b> for which
the commutative and distributive axioms of multiplication hold, ie.

|a>|B> = |b>|a>,
{cy |@,>-+ Cg|@g>}|D> = €y]ay>[b>++¢g|@2>[B>,
|a>{e, |by> +e2|b,>} = ¢,|a>|b,>+¢,|a>|be>,
﻿|
}
{
|
|

82 REPRESENTATIONS § 20

the c's being numbers. We can give a meaning to any A-variable
operating on the product |a>|b> by assuming that it operates only
on the |a> factor and commutes with the |6> factor, and similarly
we can give a meaning to any B-variable operating on this product
by assuming that it operates only on the [b> factor and commutes
with the |a> factor. (This makes every A-variable commute with
every B-variable.) Thus any dynamical variable of the original
system can operate on the product |a@>|b>, so this product can be
looked upon as a ket for the original system, and may then be
written |ab>, the two labels a and 6b being sufficient to specify it.
In this way we get the fundamental equations

|a>|b> = |b>|a> = |ab>. (65)

The multiplication here is of quite a different kind from any that
occurs earlier in the theory. The ket vectors |a@> and |b> are in two
different vector spaces and their product is in a third vector space,
which may be called the product of the two previous vector spaces.
The number of dimensions of the product space is equal to the
product of the number of dimensions of each of the factor spaces.
A general ket vector of the product space is not of the form (65), but
is a sum or integral of kets of this form.

Let us take a representation for the A-system in which a complete
set of commuting observables ¢, of the A-system are diagonal. We
shall then have the basic-bras <€!,| for the A-system. Similarly, taking
a representation for the B-system with the observables é, diagonal,
we shall have the basic bras <£,| for the B-system. The products

Eales] = <E4Es! (66)
will then provide the basic bras for a representation for the original
system, in which representation the ¢,'s and the €,'s will be diagonal.

The €,'s and €,'s will together form a complete set of commuting
observables for the original system. From (65) and (66) we get

<E4l>Eplb> = <E4 Ep lab), (67)
showing that the representative of |ab> equals the product of the
representatives of |a> and of {b> in their respective representations.

We can introduce the standard ket, >, say, for the A-system,
with respect to the representation with the €,'s diagonal, and also
the standard ket >, for the B-system, with respect to the repre-
sentation with the €,'s diagonal. Their product >, >, is then the
﻿§ 20 DEVELOPMENTS IN NOTATION 83

standard ket for the original system, with respect to the representa-
tion with the €,'s and ,'s diagonal. Any ket for the original system

may be expressed as WEs En) ab (68)

It may be that in a certain calculation we wish to use a particular
representation for the B-system, say the above representation with
the £,'s diagonal, but do not wish to introduce any particular
representation for the A-system. It would then be convenient to
use the standard ket >, for the B-system and no standard ket for
the A-system. Under these circumstances we could write any ket

for the original system as ‘Eee (69)

in which |&,> is a ket for the A-system and is also a function of the
E,'s, ie. it is a ket for the A-system for each set of values for the
£,'s—in fact (69) equals (68) if we take

\Em> = HE s Ep) a:

We may leave the standard ket >, in (69) understood, and then we
have the general ket for the original system appearing as |f;,>, a ket
for the A-system and a wave function in the variables €, of the
B-system. An example of this notation will be used in § 66.

The above work can be immediately extended to a dynamical
system describable in terms of dynamical variables which can be
divided into three or more sets A,B, C,... such that any member of
one set commutes with any member of another. Equation (65) gets

generalized to la>|b>|e>... = labe...>,

the factors on the left being kets for the component systems and
the ket on the right being a ket for the original system. Equations
(66), (67), and (68) get generalized to many factors in a similar way.

</p>
    </body>
</html>